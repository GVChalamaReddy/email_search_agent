{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":120,"sourceType":"datasetVersion","datasetId":55}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Install and Import the Required Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -U -qq langchain==0.3 langchain-openai==0.3 langchain_community==0.3 langchain-experimental==0.3 chromadb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:00:35.660628Z","iopub.execute_input":"2025-11-29T07:00:35.660790Z","iopub.status.idle":"2025-11-29T07:01:05.608196Z","shell.execute_reply.started":"2025-11-29T07:00:35.660775Z","shell.execute_reply":"2025-11-29T07:01:05.607363Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.9/206.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.5/438.5 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.6/456.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.1/456.1 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngoogle-adk 1.18.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\ngoogle-adk 1.18.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\ngoogle-adk 1.18.0 requires tenacity<10.0.0,>=9.0.0, but you have tenacity 8.5.0 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\nopentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\nfrom typing import List, Dict, Any, Generator, Optional\nimport pandas as pd\nfrom datetime import datetime\nfrom email import message_from_string\nfrom email.parser import Parser\nfrom email.policy import default\n\n# LangChain imports\nimport chromadb\nfrom chromadb.config import Settings\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document\nfrom langchain.agents import AgentExecutor, create_openai_functions_agent\nfrom langchain_openai import ChatOpenAI\nfrom langchain.tools import Tool\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import ConversationalRetrievalChain\nfrom langchain_experimental.agents import create_pandas_dataframe_agent\nfrom langchain.agents.agent_types import AgentType\nfrom langchain_community.utilities import SQLDatabase\nfrom langchain_experimental.sql import SQLDatabaseChain\nfrom langchain.agents.agent_toolkits import SQLDatabaseToolkit\nfrom langchain.agents import create_sql_agent\n\nimport sqlite3\nimport re\nimport warnings\nimport gc \n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 1000)\npd.set_option('display.max_colwidth', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:01:05.612081Z","iopub.execute_input":"2025-11-29T07:01:05.612361Z","iopub.status.idle":"2025-11-29T07:01:12.321464Z","shell.execute_reply.started":"2025-11-29T07:01:05.612336Z","shell.execute_reply":"2025-11-29T07:01:12.320679Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# API Key Placeholder: Replace 'YOUR_API_KEY_HERE' with your actual OpenAI API key.\n# The script relies on this key for the OpenAIEmbeddingFunction.\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nOPENAI_API_KEY = user_secrets.get_secret(\"OPENAI_API_KEY\")\n\n# use in google colab\n#OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_API_KEY_HERE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:01:12.322891Z","iopub.execute_input":"2025-11-29T07:01:12.323346Z","iopub.status.idle":"2025-11-29T07:01:12.428402Z","shell.execute_reply.started":"2025-11-29T07:01:12.323326Z","shell.execute_reply":"2025-11-29T07:01:12.427304Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# --- GLOBAL CONFIGURATION (Simulating OpenAI RAG Stack) ---\n# Embedding Model (Bi-Encoder): OpenAI's model for quality semantic retrieval\nEMBEDDING_MODEL_NAME = 'text-embedding-ada-002' \n# Cross-Encoder (Re-ranker): HuggingFace model for post-retrieval ranking\nRE_RANKER_MODEL_NAME = 'cross-encoder/ms-marco-MiniLM-L-6-v2' \n# LLM (Generator): OpenAI's powerful model for synthesis\nLLM_MODEL_NAME = 'gpt-3.5-turbo' \n\nPERSIST_DIRECTORY = \"./email_chroma_db\"\nCHROMA_COLLECTION_NAME = \"email_embeddings\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:01:31.819354Z","iopub.execute_input":"2025-11-29T07:01:31.819859Z","iopub.status.idle":"2025-11-29T07:01:31.823753Z","shell.execute_reply.started":"2025-11-29T07:01:31.819832Z","shell.execute_reply":"2025-11-29T07:01:31.823031Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# CSV data file path\nfile_path = \"/kaggle/input/enron-email-dataset/emails.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:01:35.117818Z","iopub.execute_input":"2025-11-29T07:01:35.118130Z","iopub.status.idle":"2025-11-29T07:01:35.123005Z","shell.execute_reply.started":"2025-11-29T07:01:35.118107Z","shell.execute_reply":"2025-11-29T07:01:35.122195Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Layer 1: The Embedding Layer (Processing, Cleaning, Chunking)","metadata":{}},{"cell_type":"markdown","source":"#### Loading and Parsing Email Data","metadata":{}},{"cell_type":"code","source":"def get_total_count():\n    \"\"\"Get total number of emails without loading entire file\"\"\"\n    try:\n        # Read only first column to count rows efficiently\n        df = pd.read_csv(file_path, usecols=[0])\n        total_emails = len(df)\n        del df\n        gc.collect()\n    except Exception as e:\n        print(f\"Warning: Could not get total count: {e}\")\n        total_emails = 0\n        \n    return total_emails\n\ntotal_emails = get_total_count()\nprint(f'Total emails - {total_emails}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:01:41.794627Z","iopub.execute_input":"2025-11-29T07:01:41.794941Z","iopub.status.idle":"2025-11-29T07:01:56.494766Z","shell.execute_reply.started":"2025-11-29T07:01:41.794920Z","shell.execute_reply":"2025-11-29T07:01:56.494141Z"}},"outputs":[{"name":"stdout","text":"Total emails - 517401\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def parse_email_message(email_text: str) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Parse individual email message using email library\n        \n        Args:\n            email_text: Raw email text\n            \n        Returns:\n            Dictionary containing parsed email fields or None if parsing fails\n        \"\"\"\n        try:\n            msg = message_from_string(email_text)\n            \n            # Extract metadata\n            email_data = {\n                'message_id': msg.get('Message-ID', ''),\n                'date': msg.get('Date', ''),\n                'from': msg.get('From', ''),\n                'to': msg.get('To', ''),\n                'subject': msg.get('Subject', ''),\n                'cc': msg.get('X-cc', ''),\n                'bcc': msg.get('X-bcc', ''),\n                'folder': msg.get('X-Folder', ''),\n                'origin': msg.get('X-Origin', ''),\n            }\n            \n            # Extract body content\n            if msg.is_multipart():\n                body = ''\n                for part in msg.walk():\n                    if part.get_content_type() == 'text/plain':\n                        payload = part.get_payload(decode=True)\n                        if payload:\n                            body += payload.decode('utf-8', errors='ignore')\n            else:\n                body = msg.get_payload(decode=True)\n                if body:\n                    body = body.decode('utf-8', errors='ignore')\n                else:\n                    body = ''\n            \n            email_data['body'] = body.strip()\n            \n            # Parse date to datetime object\n            try:\n                date_str = email_data['date'].split('(')[0].strip()\n                email_data['datetime'] = datetime.strptime(\n                    date_str,\n                    '%a, %d %b %Y %H:%M:%S %z'\n                )\n            except:\n                email_data['datetime'] = None\n            \n            return email_data\n            \n        except Exception as e:\n            print(f\"Error parsing email: {e}\")\n            return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:01:59.332015Z","iopub.execute_input":"2025-11-29T07:01:59.332293Z","iopub.status.idle":"2025-11-29T07:01:59.339810Z","shell.execute_reply.started":"2025-11-29T07:01:59.332273Z","shell.execute_reply":"2025-11-29T07:01:59.339215Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def load_emails_chunked() -> Generator[List[Dict[str, Any]], None, None]:\n        \"\"\"\n        Load and parse emails in chunks as generator\n        \n        Yields:\n            List of parsed email dictionaries for each chunk\n        \"\"\"\n        try:\n            # Read CSV in chunks\n            chunk_iterator = pd.read_csv(\n                file_path,\n                chunksize=10000\n            )\n            \n            chunk_num = 0\n            total_processed = 0\n            \n            for chunk_df in chunk_iterator:\n                chunk_num += 1\n                parsed_chunk = []\n                \n                for idx, row in chunk_df.iterrows():\n                    email_data = parse_email_message(row['message'])\n                    if email_data:\n                        email_data['file_ref'] = row['file']\n                        email_data['row_index'] = idx\n                        email_data['chunk_number'] = chunk_num\n                        parsed_chunk.append(email_data)\n                \n                total_processed += len(parsed_chunk)\n                \n                print(f\"Processed chunk {chunk_num}: {len(parsed_chunk)} emails \"\n                      f\"({total_processed}/{total_emails} total)\")\n                \n                yield parsed_chunk\n                \n                # Force garbage collection after each chunk\n                gc.collect()\n                \n        except Exception as e:\n            print(f\"Error loading emails in chunks: {e}\")\n            return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:02:02.617394Z","iopub.execute_input":"2025-11-29T07:02:02.618073Z","iopub.status.idle":"2025-11-29T07:02:02.624053Z","shell.execute_reply.started":"2025-11-29T07:02:02.618045Z","shell.execute_reply":"2025-11-29T07:02:02.623237Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def load_emails_batch(max_emails: Optional[int] = None) -> List[Dict[str, Any]]:\n        \"\"\"\n        Load emails in batches but return as single list\n        Useful when you need all emails but want memory-efficient loading\n        \n        Args:\n            max_emails: Maximum number of emails to load (None = all)\n            \n        Returns:\n            List of all parsed email dictionaries\n        \"\"\"\n        all_emails = []\n        emails_loaded = 0\n        \n        for chunk in load_emails_chunked():\n            all_emails.extend(chunk)\n            emails_loaded += len(chunk)\n            \n            if max_emails and emails_loaded >= max_emails:\n                all_emails = all_emails[:max_emails]\n                break\n        \n        print(f\"Total emails loaded: {len(all_emails)}\")\n        return all_emails\n\nall_emails = load_emails_batch(total_emails)\nemail_df = pd.DataFrame(all_emails)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:02:12.010526Z","iopub.execute_input":"2025-11-29T07:02:12.011172Z","iopub.status.idle":"2025-11-29T07:04:13.797523Z","shell.execute_reply.started":"2025-11-29T07:02:12.011148Z","shell.execute_reply":"2025-11-29T07:04:13.796902Z"}},"outputs":[{"name":"stdout","text":"Processed chunk 1: 10000 emails (10000/517401 total)\nProcessed chunk 2: 10000 emails (20000/517401 total)\nProcessed chunk 3: 10000 emails (30000/517401 total)\nProcessed chunk 4: 10000 emails (40000/517401 total)\nProcessed chunk 5: 10000 emails (50000/517401 total)\nProcessed chunk 6: 10000 emails (60000/517401 total)\nProcessed chunk 7: 10000 emails (70000/517401 total)\nProcessed chunk 8: 10000 emails (80000/517401 total)\nProcessed chunk 9: 10000 emails (90000/517401 total)\nProcessed chunk 10: 10000 emails (100000/517401 total)\nProcessed chunk 11: 10000 emails (110000/517401 total)\nProcessed chunk 12: 10000 emails (120000/517401 total)\nProcessed chunk 13: 10000 emails (130000/517401 total)\nProcessed chunk 14: 10000 emails (140000/517401 total)\nProcessed chunk 15: 10000 emails (150000/517401 total)\nProcessed chunk 16: 10000 emails (160000/517401 total)\nProcessed chunk 17: 10000 emails (170000/517401 total)\nProcessed chunk 18: 10000 emails (180000/517401 total)\nProcessed chunk 19: 10000 emails (190000/517401 total)\nProcessed chunk 20: 10000 emails (200000/517401 total)\nProcessed chunk 21: 10000 emails (210000/517401 total)\nProcessed chunk 22: 10000 emails (220000/517401 total)\nProcessed chunk 23: 10000 emails (230000/517401 total)\nProcessed chunk 24: 10000 emails (240000/517401 total)\nProcessed chunk 25: 10000 emails (250000/517401 total)\nProcessed chunk 26: 10000 emails (260000/517401 total)\nProcessed chunk 27: 10000 emails (270000/517401 total)\nProcessed chunk 28: 10000 emails (280000/517401 total)\nProcessed chunk 29: 10000 emails (290000/517401 total)\nProcessed chunk 30: 10000 emails (300000/517401 total)\nProcessed chunk 31: 10000 emails (310000/517401 total)\nProcessed chunk 32: 10000 emails (320000/517401 total)\nProcessed chunk 33: 10000 emails (330000/517401 total)\nProcessed chunk 34: 10000 emails (340000/517401 total)\nProcessed chunk 35: 10000 emails (350000/517401 total)\nProcessed chunk 36: 10000 emails (360000/517401 total)\nProcessed chunk 37: 10000 emails (370000/517401 total)\nProcessed chunk 38: 10000 emails (380000/517401 total)\nProcessed chunk 39: 10000 emails (390000/517401 total)\nProcessed chunk 40: 10000 emails (400000/517401 total)\nProcessed chunk 41: 10000 emails (410000/517401 total)\nProcessed chunk 42: 10000 emails (420000/517401 total)\nProcessed chunk 43: 10000 emails (430000/517401 total)\nProcessed chunk 44: 10000 emails (440000/517401 total)\nProcessed chunk 45: 10000 emails (450000/517401 total)\nProcessed chunk 46: 10000 emails (460000/517401 total)\nProcessed chunk 47: 10000 emails (470000/517401 total)\nProcessed chunk 48: 10000 emails (480000/517401 total)\nProcessed chunk 49: 10000 emails (490000/517401 total)\nProcessed chunk 50: 10000 emails (500000/517401 total)\nProcessed chunk 51: 10000 emails (510000/517401 total)\nProcessed chunk 52: 7401 emails (517401/517401 total)\nTotal emails loaded: 517401\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"email_df.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:04:42.622073Z","iopub.execute_input":"2025-11-29T07:04:42.622369Z","iopub.status.idle":"2025-11-29T07:04:42.661100Z","shell.execute_reply.started":"2025-11-29T07:04:42.622349Z","shell.execute_reply":"2025-11-29T07:04:42.660311Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                      message_id                                   date                     from                       to    subject cc bcc                                                 folder   origin  \\\n0  <18782981.1075855378110.JavaMail.evans@thyme>  Mon, 14 May 2001 16:39:00 -0700 (PDT)  phillip.allen@enron.com     tim.belden@enron.com                    \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail  Allen-P   \n1  <15464986.1075855378456.JavaMail.evans@thyme>   Fri, 4 May 2001 13:51:00 -0700 (PDT)  phillip.allen@enron.com  john.lavorato@enron.com        Re:         \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail  Allen-P   \n2  <24216240.1075855687451.JavaMail.evans@thyme>  Wed, 18 Oct 2000 03:00:00 -0700 (PDT)  phillip.allen@enron.com   leah.arsdall@enron.com   Re: test               \\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail  Allen-P   \n3  <13505866.1075863688222.JavaMail.evans@thyme>  Mon, 23 Oct 2000 06:13:00 -0700 (PDT)  phillip.allen@enron.com    randall.gay@enron.com                          \\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail  Allen-P   \n4  <30922949.1075863688243.JavaMail.evans@thyme>  Thu, 31 Aug 2000 05:07:00 -0700 (PDT)  phillip.allen@enron.com     greg.piper@enron.com  Re: Hello               \\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail  Allen-P   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    body                   datetime                  file_ref  row_index  chunk_number  \n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Here is our forecast  2001-05-14 16:39:00-07:00     allen-p/_sent_mail/1.          0             1  \n1  Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\\n\\nAs far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \\n\\nMy suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.  2001-05-04 13:51:00-07:00    allen-p/_sent_mail/10.          1             1  \n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         test successful.  way to go!!!  2000-10-18 03:00:00-07:00   allen-p/_sent_mail/100.          2             1  \n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Randy,\\n\\n Can you send me a schedule of the salary and level of everyone in the \\nscheduling group.  Plus your thoughts on any changes that need to be made.  \\n(Patti S for example)\\n\\nPhillip  2000-10-23 06:13:00-07:00  allen-p/_sent_mail/1000.          3             1  \n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Let's shoot for Tuesday at 11:45.  2000-08-31 05:07:00-07:00  allen-p/_sent_mail/1001.          4             1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>message_id</th>\n      <th>date</th>\n      <th>from</th>\n      <th>to</th>\n      <th>subject</th>\n      <th>cc</th>\n      <th>bcc</th>\n      <th>folder</th>\n      <th>origin</th>\n      <th>body</th>\n      <th>datetime</th>\n      <th>file_ref</th>\n      <th>row_index</th>\n      <th>chunk_number</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;18782981.1075855378110.JavaMail.evans@thyme&gt;</td>\n      <td>Mon, 14 May 2001 16:39:00 -0700 (PDT)</td>\n      <td>phillip.allen@enron.com</td>\n      <td>tim.belden@enron.com</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail</td>\n      <td>Allen-P</td>\n      <td>Here is our forecast</td>\n      <td>2001-05-14 16:39:00-07:00</td>\n      <td>allen-p/_sent_mail/1.</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;15464986.1075855378456.JavaMail.evans@thyme&gt;</td>\n      <td>Fri, 4 May 2001 13:51:00 -0700 (PDT)</td>\n      <td>phillip.allen@enron.com</td>\n      <td>john.lavorato@enron.com</td>\n      <td>Re:</td>\n      <td></td>\n      <td></td>\n      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Sent Mail</td>\n      <td>Allen-P</td>\n      <td>Traveling to have a business meeting takes the fun out of the trip.  Especially if you have to prepare a presentation.  I would suggest holding the business plan meetings here then take a trip without any formal business meetings.  I would even try and get some honest opinions on whether a trip is even desired or necessary.\\n\\nAs far as the business meetings, I think it would be more productive to try and stimulate discussions across the different groups about what is working and what is not.  Too often the presenter speaks and the others are quiet just waiting for their turn.   The meetings might be better if held in a round table discussion format.  \\n\\nMy suggestion for where to go is Austin.  Play golf and rent a ski boat and jet ski's.  Flying somewhere takes too much time.</td>\n      <td>2001-05-04 13:51:00-07:00</td>\n      <td>allen-p/_sent_mail/10.</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;24216240.1075855687451.JavaMail.evans@thyme&gt;</td>\n      <td>Wed, 18 Oct 2000 03:00:00 -0700 (PDT)</td>\n      <td>phillip.allen@enron.com</td>\n      <td>leah.arsdall@enron.com</td>\n      <td>Re: test</td>\n      <td></td>\n      <td></td>\n      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n      <td>Allen-P</td>\n      <td>test successful.  way to go!!!</td>\n      <td>2000-10-18 03:00:00-07:00</td>\n      <td>allen-p/_sent_mail/100.</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;13505866.1075863688222.JavaMail.evans@thyme&gt;</td>\n      <td>Mon, 23 Oct 2000 06:13:00 -0700 (PDT)</td>\n      <td>phillip.allen@enron.com</td>\n      <td>randall.gay@enron.com</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n      <td>Allen-P</td>\n      <td>Randy,\\n\\n Can you send me a schedule of the salary and level of everyone in the \\nscheduling group.  Plus your thoughts on any changes that need to be made.  \\n(Patti S for example)\\n\\nPhillip</td>\n      <td>2000-10-23 06:13:00-07:00</td>\n      <td>allen-p/_sent_mail/1000.</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;30922949.1075863688243.JavaMail.evans@thyme&gt;</td>\n      <td>Thu, 31 Aug 2000 05:07:00 -0700 (PDT)</td>\n      <td>phillip.allen@enron.com</td>\n      <td>greg.piper@enron.com</td>\n      <td>Re: Hello</td>\n      <td></td>\n      <td></td>\n      <td>\\Phillip_Allen_Dec2000\\Notes Folders\\'sent mail</td>\n      <td>Allen-P</td>\n      <td>Let's shoot for Tuesday at 11:45.</td>\n      <td>2000-08-31 05:07:00-07:00</td>\n      <td>allen-p/_sent_mail/1001.</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def get_email_text(email: Dict[str, Any]) -> str:\n    \"\"\"\n    Create full-text representation of email for embedding\n    \n    Args:\n        email: Parsed email dictionary\n        \n    Returns:\n        Formatted email text\n    \"\"\"\n    text_parts = [\n        f\"From: {email.get('from', 'Unknown')}\",\n        f\"To: {email.get('to', 'Unknown')}\",\n        f\"Date: {email.get('date', 'Unknown')}\",\n        f\"Subject: {email.get('subject', 'No Subject')}\",\n    ]\n    \n    if email.get('cc'):\n        text_parts.append(f\"CC: {email['cc']}\")\n    \n    text_parts.append(f\"\\n{email.get('body', '')}\")\n    \n    return \"\\n\".join(text_parts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:04:48.356606Z","iopub.execute_input":"2025-11-29T07:04:48.357204Z","iopub.status.idle":"2025-11-29T07:04:48.376328Z","shell.execute_reply.started":"2025-11-29T07:04:48.357177Z","shell.execute_reply":"2025-11-29T07:04:48.375568Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"#### Email Data Chunking ","metadata":{}},{"cell_type":"code","source":"import tiktoken\n\nencoding = tiktoken.encoding_for_model(LLM_MODEL_NAME)\n\ndef token_length(text: str) -> int:\n    \"\"\"\n    Count tokens in text\n    \n    Args:\n        text: Input text\n        \n    Returns:\n        Number of tokens\n    \"\"\"\n    tokens = encoding.encode(text)\n    return len(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:04:55.497210Z","iopub.execute_input":"2025-11-29T07:04:55.497467Z","iopub.status.idle":"2025-11-29T07:04:56.128475Z","shell.execute_reply.started":"2025-11-29T07:04:55.497448Z","shell.execute_reply":"2025-11-29T07:04:56.127909Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Configure text splitter for chunking long emails\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,          # Max chars per chunk\n    chunk_overlap=200,        # Overlap to maintain context\n    length_function=token_length,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Split on paragraphs first\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:04:56.463390Z","iopub.execute_input":"2025-11-29T07:04:56.463660Z","iopub.status.idle":"2025-11-29T07:04:56.467853Z","shell.execute_reply.started":"2025-11-29T07:04:56.463642Z","shell.execute_reply":"2025-11-29T07:04:56.467286Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def chunk_email(email_text: str, metadata: Dict[str, Any]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Chunk a single email into smaller pieces\n    \n    Args:\n        email_text: Full email text\n        metadata: Email metadata to attach to each chunk\n        \n    Returns:\n        List of chunk dictionaries with text and metadata\n    \"\"\"\n    chunks = text_splitter.split_text(email_text)\n    \n    chunk_dicts = []\n    for idx, chunk in enumerate(chunks):\n        chunk_dict = {\n            'text': chunk,\n            'chunk_index': idx,\n            'total_chunks': len(chunks),\n            **metadata\n        }\n        chunk_dicts.append(chunk_dict)\n    \n    return chunk_dicts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:04:57.878092Z","iopub.execute_input":"2025-11-29T07:04:57.878750Z","iopub.status.idle":"2025-11-29T07:04:57.882808Z","shell.execute_reply.started":"2025-11-29T07:04:57.878718Z","shell.execute_reply":"2025-11-29T07:04:57.882236Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def chunk_emails_streaming(email_chunks: Generator[List[Dict[str, Any]], None, None],\n                           get_text_fn) -> Generator[List[Dict[str, Any]], None, None]:\n    \"\"\"\n    Chunk multiple emails in streaming fashion\n    Processes email chunks as they come without loading all into memory\n    \n    Args:\n        email_chunks: Generator yielding email chunk lists\n        get_text_fn: Function to extract text from email dict\n        \n    Yields:\n        Lists of text chunks with metadata\n    \"\"\"\n    chunk_count = 0\n    \n    for email_batch in email_chunks:\n        batch_chunks = []\n        \n        for email in email_batch:\n            email_text = get_text_fn(email)\n            \n            # Create metadata for chunks\n            metadata = {\n                'message_id': email.get('message_id', ''),\n                'from': email.get('from', ''),\n                'to': email.get('to', ''),\n                    'subject': email.get('subject', ''),\n                    'date': email.get('date', ''),\n                    'file_ref': email.get('file_ref', ''),\n                    'row_index': email.get('row_index', -1),\n                    'chunk_number': email.get('chunk_number', 0)\n                }\n                \n            # Chunk the email\n            chunks = chunk_email(email_text, metadata)\n            batch_chunks.extend(chunks)\n            chunk_count += len(chunks)\n            \n        print(f\"Generated {len(batch_chunks)} text chunks (total: {chunk_count})\")\n        \n        yield batch_chunks\n        \n        # Force garbage collection\n        gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:04:59.450637Z","iopub.execute_input":"2025-11-29T07:04:59.450902Z","iopub.status.idle":"2025-11-29T07:04:59.456685Z","shell.execute_reply.started":"2025-11-29T07:04:59.450855Z","shell.execute_reply":"2025-11-29T07:04:59.456038Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Layer 2: The Search Layer (Retrieval, Re-ranking, & Caching)","metadata":{}},{"cell_type":"markdown","source":"#### Manage Embeddings and Semanic Search using ChromaDB","metadata":{}},{"cell_type":"code","source":"# Initialize OpenAI embeddings\nembeddings = OpenAIEmbeddings(\n    openai_api_key=OPENAI_API_KEY,\n    model=EMBEDDING_MODEL_NAME\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:03.866343Z","iopub.execute_input":"2025-11-29T07:05:03.867144Z","iopub.status.idle":"2025-11-29T07:05:04.320452Z","shell.execute_reply.started":"2025-11-29T07:05:03.867116Z","shell.execute_reply":"2025-11-29T07:05:04.319905Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"chromadb.api.client.SharedSystemClient.clear_system_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:04.706194Z","iopub.execute_input":"2025-11-29T07:05:04.706437Z","iopub.status.idle":"2025-11-29T07:05:04.710275Z","shell.execute_reply.started":"2025-11-29T07:05:04.706418Z","shell.execute_reply":"2025-11-29T07:05:04.709627Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Initialize ChromaDB client\nclient = chromadb.PersistentClient(\n    path=PERSIST_DIRECTORY,\n    settings=Settings(anonymized_telemetry=False)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:05.858277Z","iopub.execute_input":"2025-11-29T07:05:05.858529Z","iopub.status.idle":"2025-11-29T07:05:06.101135Z","shell.execute_reply.started":"2025-11-29T07:05:05.858511Z","shell.execute_reply":"2025-11-29T07:05:06.100574Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def create_vectorstore_streaming(chunk_generator: Generator[List[Dict[str, Any]], None, None]):\n        \"\"\"\n        Create vector store from streaming chunks\n        More memory-efficient for large datasets\n        \n        Args:\n            chunk_generator: Generator yielding chunk batches\n        \"\"\"\n        print(f\"Creating vector store with streaming ingestion...\")\n        \n        total_chunks = 0\n        first_batch = True\n        \n        for chunk_batch in chunk_generator:\n            # Extract texts and metadata\n            texts = [chunk['text'] for chunk in chunk_batch]\n            metadatas = [\n                {\n                    'message_id': chunk.get('message_id', ''),\n                    'from': chunk.get('from', ''),\n                    'to': chunk.get('to', ''),\n                    'subject': chunk.get('subject', ''),\n                    'date': chunk.get('date', ''),\n                    'chunk_index': str(chunk.get('chunk_index', 0)),\n                    'total_chunks': str(chunk.get('total_chunks', 1)),\n                    'file_ref': chunk.get('file_ref', ''),\n                    'row_index': str(chunk.get('row_index', -1))\n                }\n                for chunk in chunk_batch\n            ]\n            \n            # Create or add to vector store\n            if first_batch:\n                # Create initial vectorstore\n                vector_store = Chroma.from_texts(\n                    texts=texts,\n                    embedding=embeddings,\n                    metadatas=metadatas,\n                    collection_name=CHROMA_COLLECTION_NAME,\n                    client=client\n                )\n                first_batch = False\n            else:\n                # Add to existing vectorstore\n                vector_store.add_texts(\n                    texts=texts,\n                    metadatas=metadatas\n                )\n            \n            total_chunks += len(texts)\n            print(f\"Indexed {len(texts)} chunks (total: {total_chunks})\")\n            \n            # Force garbage collection\n            gc.collect()\n        \n        print(f\"Vector store created successfully with {total_chunks} chunks!\")\n        return vector_store","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:07.570542Z","iopub.execute_input":"2025-11-29T07:05:07.571243Z","iopub.status.idle":"2025-11-29T07:05:07.577525Z","shell.execute_reply.started":"2025-11-29T07:05:07.571219Z","shell.execute_reply":"2025-11-29T07:05:07.576930Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def load_vectorstore():\n    \"\"\"\n    Load existing vector store from disk.\n    \n    Returns:\n        Chroma vectorstore instance\n        \n    Raises:\n        FileNotFoundError: If vector store doesn't exist\n    \"\"\"\n    if os.path.exists(PERSIST_DIRECTORY):\n        print(f\"Loading existing vector store from {PERSIST_DIRECTORY}...\")\n        vector_store = Chroma(\n            client=client,\n            collection_name=CHROMA_COLLECTION_NAME,\n            embedding_function=embeddings\n        )\n        print(\"✓ Vector store loaded successfully\")\n        return vector_store\n    else:\n        print(f\"Vector store not found at {PERSIST_DIRECTORY}. Please build the vector store first.\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:09.244032Z","iopub.execute_input":"2025-11-29T07:05:09.244619Z","iopub.status.idle":"2025-11-29T07:05:09.249072Z","shell.execute_reply.started":"2025-11-29T07:05:09.244598Z","shell.execute_reply":"2025-11-29T07:05:09.248284Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def similarity_search(query: str, k: int = 5) -> List[Dict[str, Any]]:\n    \"\"\"\n    Perform similarity search\n    \n    Args:\n        query: Search query\n        k: Number of results to return\n        \n    Returns:\n        List of similar documents with metadata\n    \"\"\"\n    if vector_store is None:\n        raise ValueError(\n            \"Vector store not initialized. \"\n            \"Please build or load the vector store first.\"\n        )\n    \n    results = vector_store.similarity_search_with_score(query, k=k)\n    \n    formatted_results = []\n    for doc, score in results:\n        formatted_results.append({\n            'content': doc.page_content,\n            'metadata': doc.metadata,\n            'similarity_score': score\n        })\n    \n    return formatted_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:10.800080Z","iopub.execute_input":"2025-11-29T07:05:10.800345Z","iopub.status.idle":"2025-11-29T07:05:10.804978Z","shell.execute_reply.started":"2025-11-29T07:05:10.800326Z","shell.execute_reply":"2025-11-29T07:05:10.804264Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Check if we need to index emails\nif not os.path.exists(PERSIST_DIRECTORY):\n    print(\"\\n[STREAMING MODE] Indexing emails ...\")\n    \n    print(f\"\\n Email Parser Configuration:\")\n    print(f\"  Total Emails: {total_emails}\")\n    print(f\"  Chunk Size: {10000} emails per chunk\")\n    \n    # Stream emails -> chunks -> embeddings\n    print(\"\\n[PHASE 1] Parsing emails in chunks...\")\n    email_chunks_gen = load_emails_chunked()\n    \n    \n    print(f\"\\n Email Data Chunking Configuration:\")\n    print(f\"  Text Chunk Size: {1000} tokens\")\n    print(f\"  Overlap: {200} tokens\")\n    \n    print(\"\\n[PHASE 2] Generating text chunks...\")\n    text_chunks_gen = chunk_emails_streaming(\n        email_chunks_gen,\n        get_email_text\n    )\n    \n    print(\"\\n[PHASE 3] Creating embeddings and indexing...\")\n    vector_store = create_vectorstore_streaming(text_chunks_gen)\n    \n    # Force garbage collection\n    gc.collect()\n        \nelse:\n    print(\"\\n[LOADING] Loading existing vector store...\")\n    vector_store = load_vectorstore()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:12.788960Z","iopub.execute_input":"2025-11-29T07:05:12.789643Z","iopub.status.idle":"2025-11-29T07:05:12.808629Z","shell.execute_reply.started":"2025-11-29T07:05:12.789622Z","shell.execute_reply":"2025-11-29T07:05:12.807895Z"}},"outputs":[{"name":"stdout","text":"\n[LOADING] Loading existing vector store...\nLoading existing vector store from ./email_chroma_db...\n✓ Vector store loaded successfully\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Layer 3: The Generation Layer (LLM Simulation)","metadata":{}},{"cell_type":"markdown","source":"#### LangChain-powered Intelligent Agents for Email Search","metadata":{}},{"cell_type":"code","source":"#LLM model to use (gpt-3.5-turbo)\nllm = ChatOpenAI(\n            temperature=0,                    # Deterministic responses\n            model=LLM_MODEL_NAME,\n            openai_api_key=OPENAI_API_KEY\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:25.116487Z","iopub.execute_input":"2025-11-29T07:05:25.117071Z","iopub.status.idle":"2025-11-29T07:05:25.221587Z","shell.execute_reply.started":"2025-11-29T07:05:25.117029Z","shell.execute_reply":"2025-11-29T07:05:25.220861Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"#### Conversational Agent","metadata":{}},{"cell_type":"code","source":"# Initialize conversation memory\nmemory = ConversationBufferMemory(\n    memory_key=\"chat_history\",\n    return_messages=True,\n    output_key=\"answer\"\n)\n\n# Custom prompt for conversational context\nqa_prompt = PromptTemplate(\n    template=\"\"\"You are a helpful and friendly email search assistant. You engage in natural conversations while helping users find information in their email archives.\n\nPrevious conversation:\n{chat_history}\n\nContext from emails:\n{context}\n\nCurrent question: {question}\n\nProvide a conversational, helpful response that:\n1. Acknowledges previous conversation context when relevant\n2. Directly answers the current question\n3. References specific emails naturally (e.g., \"In the email from John on May 14th...\")\n4. Asks clarifying questions if needed\n5. Maintains a friendly, professional tone\n\nAnswer:\"\"\",\n            input_variables=[\"chat_history\", \"context\", \"question\"]\n        )\n\n\n# Create conversational retrieval chain\nconversation_chain = ConversationalRetrievalChain.from_llm(\n    llm=llm,\n    retriever=vector_store.as_retriever(\n        search_kwargs={\"k\": 5}\n    ),\n    memory=memory,\n    return_source_documents=True,\n    combine_docs_chain_kwargs={\"prompt\": qa_prompt}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:26.889787Z","iopub.execute_input":"2025-11-29T07:05:26.890502Z","iopub.status.idle":"2025-11-29T07:05:26.895570Z","shell.execute_reply.started":"2025-11-29T07:05:26.890479Z","shell.execute_reply":"2025-11-29T07:05:26.894918Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def conversation_chat(message: str) -> Dict[str, Any]:\n    \"\"\"\n    Have a conversation turn\n    \n    Args:\n        message: User message\n        \n    Returns:\n        Dictionary with response and sources\n    \"\"\"\n    \n    try:\n        result = conversation_chain({\"question\": message})\n        # For ConversationalRetrievalChain, the default output key is \"answer\"\n        # unless you override output_key.\n        answer = result.get(\"answer\", result.get(\"result\", \"\"))\n        response = {\n            \"status\": \"success\",\n            \"message\": message,\n            \"response\": answer,\n            \"raw\": result,\n            \"source_documents\": [],\n        }\n        for doc in result.get(\"source_documents\", []):\n            source = {\n                \"content\": doc.page_content[:300] + \"...\",\n                \"from\": doc.metadata.get(\"from\", \"Unknown\"),\n                \"to\": doc.metadata.get(\"to\", \"Unknown\"),\n                \"subject\": doc.metadata.get(\"subject\", \"No Subject\"),\n                \"date\": doc.metadata.get(\"date\", \"Unknown\"),\n            }\n            response[\"source_documents\"].append(source)\n        return response\n\n    except Exception as e:\n        # Always return \"response\" key so callers don't KeyError\n        return {\n            \"status\": \"error\",\n            \"message\": message,\n            \"response\": f\"Error in conversational agent: {e}\",\n            \"raw\": None,\n            \"source_documents\": [],\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:28.408310Z","iopub.execute_input":"2025-11-29T07:05:28.408896Z","iopub.status.idle":"2025-11-29T07:05:28.414713Z","shell.execute_reply.started":"2025-11-29T07:05:28.408857Z","shell.execute_reply":"2025-11-29T07:05:28.413921Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def get_conversation_history() -> List[Dict[str, str]]:\n    \"\"\"\n    Get conversation history\n    \n    Returns:\n        List of conversation turns\n    \"\"\"\n    history = []\n    if hasattr(memory, 'chat_memory'):\n        for message in memory.chat_memory.messages:\n            history.append({\n                'role': message.type,\n                'content': message.content\n            })\n    return history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:30.022887Z","iopub.execute_input":"2025-11-29T07:05:30.023182Z","iopub.status.idle":"2025-11-29T07:05:30.027596Z","shell.execute_reply.started":"2025-11-29T07:05:30.023161Z","shell.execute_reply":"2025-11-29T07:05:30.026811Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"#### Data Analytics Agent","metadata":{}},{"cell_type":"code","source":" def create_analytics_dataframe() -> pd.DataFrame:\n    \"\"\"\n    Create structured DataFrame from raw email data\n    \n    Returns:\n        Processed DataFrame with email fields\n    \"\"\"\n    print(\"Creating analytics DataFrame...\")\n    \n    df = email_df\n    \n    # Feature engineering\n    df['body_length'] = df['body'].str.len()\n    df['subject_length'] = df['subject'].str.len()\n    df['has_subject'] = df['subject'].str.len() > 0\n    df['has_cc'] = df['cc'].str.len() > 0\n    \n    # Extract sender domain\n    df['sender_domain'] = df['from'].str.extract(r'@([^\\s>]+)')\n    \n    # Extract date components (simplified)\n    df['date_str'] = df['date'].astype(str)\n    \n    print(f\"Analytics DataFrame created with {len(df)} emails and {len(df.columns)} features\")\n        \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:38.947978Z","iopub.execute_input":"2025-11-29T07:05:38.948227Z","iopub.status.idle":"2025-11-29T07:05:38.953552Z","shell.execute_reply.started":"2025-11-29T07:05:38.948210Z","shell.execute_reply":"2025-11-29T07:05:38.952687Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Create pandas agent\ndf = create_analytics_dataframe()\nanalytics_agent = create_pandas_dataframe_agent(\n    llm,\n    df,\n    agent_type=AgentType.OPENAI_FUNCTIONS,\n    verbose=True,\n    allow_dangerous_code=True,  # Required for pandas agent\n    handle_parsing_errors=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:40.971394Z","iopub.execute_input":"2025-11-29T07:05:40.971657Z","iopub.status.idle":"2025-11-29T07:05:43.049715Z","shell.execute_reply.started":"2025-11-29T07:05:40.971637Z","shell.execute_reply":"2025-11-29T07:05:43.049113Z"}},"outputs":[{"name":"stdout","text":"Creating analytics DataFrame...\nAnalytics DataFrame created with 517401 emails and 20 features\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"def analyze_emails(query: str) -> Dict[str, Any]:\n    \"\"\"\n    Run analytics query\n    \n    Args:\n        query: Natural language analytics query\n        \n    Returns:\n        Analysis results\n    \"\"\"\n    try:\n        result = analytics_agent.run(query)\n        \n        return {\n            'query': query,\n            'result': result,\n            'status': 'success'\n        }\n    except Exception as e:\n        return {\n            'query': query,\n            'result': f\"Error: {str(e)}\",\n            'status': 'error'\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:57.308728Z","iopub.execute_input":"2025-11-29T07:05:57.309327Z","iopub.status.idle":"2025-11-29T07:05:57.313634Z","shell.execute_reply.started":"2025-11-29T07:05:57.309300Z","shell.execute_reply":"2025-11-29T07:05:57.312977Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"#### SQL Agent","metadata":{}},{"cell_type":"code","source":"sql_db_path = \"./emails.db\"\ndef create_database():\n    \"\"\"Create SQLite database from email CSV\"\"\"\n    print(f\"Creating SQLite database at {sql_db_path}...\")\n    \n    df = email_df\n    \n    # Create SQLite database\n    conn = sqlite3.connect(sql_db_path)\n    \n    # Create main emails table\n    df.to_sql('emails', conn, if_exists='replace', index=False)\n    \n    # Create additional analytics tables\n    \n    # Senders table\n    senders_df = df[['from']].drop_duplicates()\n    senders_df['sender_id'] = range(1, len(senders_df) + 1)\n    senders_df.to_sql('senders', conn, if_exists='replace', index=False)\n        \n    # Recipients table\n    recipients_df = df[['to']].drop_duplicates()\n    recipients_df['recipient_id'] = range(1, len(recipients_df) + 1)\n    recipients_df.to_sql('recipients', conn, if_exists='replace', index=False)\n    \n    # Create indexes\n    cursor = conn.cursor()\n    cursor.execute('CREATE INDEX IF NOT EXISTS idx_from ON emails(\"from\")')\n    cursor.execute('CREATE INDEX IF NOT EXISTS idx_to ON emails(\"to\")')\n    cursor.execute('CREATE INDEX IF NOT EXISTS idx_subject ON emails(\"subject\")')\n    cursor.execute('CREATE INDEX IF NOT EXISTS idx_date ON emails(\"date\")')\n    \n    conn.commit()\n    conn.close()\n    \n    print(f\"Database created with {len(df)} emails\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:05:59.586903Z","iopub.execute_input":"2025-11-29T07:05:59.587547Z","iopub.status.idle":"2025-11-29T07:05:59.593285Z","shell.execute_reply.started":"2025-11-29T07:05:59.587527Z","shell.execute_reply":"2025-11-29T07:05:59.592613Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Create SQLite database from emails\ncreate_database()\n\n# Initialize LangChain SQL database wrapper\ndb = SQLDatabase.from_uri(f\"sqlite:///{sql_db_path}\")\n\n# Create SQL toolkit\ntoolkit = SQLDatabaseToolkit(db=db, llm=llm)\n\n# Create SQL agent\nsql_agent = create_sql_agent(\n    llm=llm,\n    toolkit=toolkit,\n    agent_type=AgentType.OPENAI_FUNCTIONS,\n    verbose=False,\n    handle_parsing_errors=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:06:01.211822Z","iopub.execute_input":"2025-11-29T07:06:01.212100Z","iopub.status.idle":"2025-11-29T07:06:19.380788Z","shell.execute_reply.started":"2025-11-29T07:06:01.212082Z","shell.execute_reply":"2025-11-29T07:06:19.379278Z"}},"outputs":[{"name":"stdout","text":"Creating SQLite database at ./emails.db...\nDatabase created with 517401 emails\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"def sql_query(question: str) -> Dict[str, Any]:\n    \"\"\"\n    Execute SQL query via natural language\n    \n    Args:\n        question: Natural language question\n        \n    Returns:\n        Query results\n    \"\"\"\n    try:        \n        raw_result = sql_agent.invoke({\"input\": question})\n            \n        return {\n            'question': question,\n            'result': raw_result['output'],\n            'intermediate_steps': raw_result.get('intermediate_steps', []),\n            'status': 'success'\n        }\n    except Exception as e:\n        return {\n            'question': question,\n            'result': f\"SQL Agent Error: {str(e)}\",\n            'status': 'error'\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:06:58.164670Z","iopub.execute_input":"2025-11-29T07:06:58.165222Z","iopub.status.idle":"2025-11-29T07:06:58.169899Z","shell.execute_reply.started":"2025-11-29T07:06:58.165197Z","shell.execute_reply":"2025-11-29T07:06:58.169074Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import sqlite3\ndef execute_raw_sql(sql: str) -> pd.DataFrame:\n    \"\"\"\n    Execute raw SQL query\n    \n    Args:\n        sql: SQL query string\n        \n    Returns:\n        Query results as DataFrame\n    \"\"\"\n    conn = sqlite3.connect(sql_db_path)\n    df = pd.read_sql_query(sql, conn)\n    conn.close()\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:06:59.875742Z","iopub.execute_input":"2025-11-29T07:06:59.876654Z","iopub.status.idle":"2025-11-29T07:06:59.880610Z","shell.execute_reply.started":"2025-11-29T07:06:59.876626Z","shell.execute_reply":"2025-11-29T07:06:59.879857Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"### Multi Agent Orchestrator","metadata":{}},{"cell_type":"code","source":"# Types of available agents\nagents = {\n        \"CONVERSATIONAL\": \"conversation_chain\",\n        \"ANALYTICS\": \"analytics_agent\",\n        \"SQL\": \"sql_agent\"\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:07:02.183425Z","iopub.execute_input":"2025-11-29T07:07:02.184311Z","iopub.status.idle":"2025-11-29T07:07:02.188198Z","shell.execute_reply.started":"2025-11-29T07:07:02.184280Z","shell.execute_reply":"2025-11-29T07:07:02.187288Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def route_query(query: str, agent_type: str) -> Dict[str, Any]:\n        \"\"\"\n        Route query to appropriate agent\n        \n        Args:\n            query: User query\n            agent_type: Specific agent to use\n            \n        Returns:\n            Agent response\n        \"\"\"\n        # Use specified agent \n        target_agent_type = agent_type\n        agent = agents.get(target_agent_type)\n        \n        if not agent:\n            return {\n                'error': f'Agent {target_agent_type} not available',\n                'status': 'error'\n            }\n        \n        # Route to appropriate agent\n        try:\n            if target_agent_type == \"CONVERSATIONAL\":\n                return conversation_chat(query)\n            elif target_agent_type == \"ANALYTICS\":\n                return analyze_emails(query)\n            elif target_agent_type == \"SQL\":\n                return execute_raw_sql(query)\n        except Exception as e:\n            return {\n                'error': str(e),\n                'status': 'error'\n            }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:07:04.678034Z","iopub.execute_input":"2025-11-29T07:07:04.678285Z","iopub.status.idle":"2025-11-29T07:07:04.684117Z","shell.execute_reply.started":"2025-11-29T07:07:04.678269Z","shell.execute_reply":"2025-11-29T07:07:04.683244Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def get_agent_info() -> Dict[str, Any]:\n    \"\"\"\n    Get information about available agents\n    \n    Returns:\n        Agent availability and capabilities\n    \"\"\"\n    return {\n        'available_agents': [\n            agent_type\n            for agent_type, agent in agents.items() \n            if agent is not None\n        ],\n        'capabilities': {\n            'conversational': 'Natural conversation with memory',\n            'analytics': 'Data analysis with pandas',\n            'sql': 'SQL queries and structured data retrieval'\n        }\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:07:06.319395Z","iopub.execute_input":"2025-11-29T07:07:06.319947Z","iopub.status.idle":"2025-11-29T07:07:06.324505Z","shell.execute_reply.started":"2025-11-29T07:07:06.319913Z","shell.execute_reply":"2025-11-29T07:07:06.323735Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Test 1: Conversational Agent\nprint(\"\\n\" + \"─\"*80)\nprint(\"TEST 1: CONVERSATIONAL AGENT\")\nprint(\"─\"*80)\n\nconv_queries = [\n    \"Hi! Can you help me search my emails?\",\n    \"I'm looking for emails about business meetings or travel plans.\",\n    \"What about salary discussions?\"\n]\n\nfor query in conv_queries:\n    print(f\"\\nUser: {query}\")\n    result = route_query(query, \"CONVERSATIONAL\")\n   \n    text = result.get(\"response\", result.get(\"answer\", result))\n    print(f\"Agent: {text}\\n\")\n\n    if result.get(\"source_documents\"):\n        print(f\"  [Referenced {len(result['source_documents'])} emails]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:07:43.555136Z","iopub.execute_input":"2025-11-29T07:07:43.555633Z","iopub.status.idle":"2025-11-29T07:07:52.305443Z","shell.execute_reply.started":"2025-11-29T07:07:43.555613Z","shell.execute_reply":"2025-11-29T07:07:52.304667Z"}},"outputs":[{"name":"stdout","text":"\n────────────────────────────────────────────────────────────────────────────────\nTEST 1: CONVERSATIONAL AGENT\n────────────────────────────────────────────────────────────────────────────────\n\nUser: Hi! Can you help me search my emails?\nAgent: Certainly! To narrow down the search for emails related to salary discussions, could you provide me with any specific details or keywords to help me find the relevant emails? For example, do you remember any specific dates, senders, or topics discussed in those emails? This information will help me locate the emails more efficiently. Feel free to share any details you remember, and I'll do my best to assist you further!\n\n\nUser: I'm looking for emails about business meetings or travel plans.\nAgent: Certainly! If you're interested in emails related to salary discussions, I can help you search for those as well. Do you remember any specific details or keywords that could help narrow down the search? For example, were there any particular dates, senders, or topics mentioned in those emails that you recall? Feel free to provide any additional information to assist me in finding the relevant emails for you. I'm here to help!\n\n\nUser: What about salary discussions?\nAgent: Certainly! I can help you search for emails related to salary discussions as well. Do you recall any specific details or keywords that could assist in narrowing down the search? For instance, were there any particular dates, senders, or topics mentioned in those emails that you remember? Feel free to provide any additional information to help me locate the relevant emails for you. I'm here to assist you in finding the information you need!\n\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Test 2: Analytics Agent\nprint(\"\\n\" + \"─\"*80)\nprint(\"TEST 2: DATA ANALYTICS AGENT\")\nprint(\"─\"*80)\n\nanalytics_queries = [\n    \"How many emails are in the dataset?\",\n    \"What is the average email body length?\",\n    \"Show me the top 3 most frequent senders\"\n]\n\nfor query in analytics_queries:\n    print(f\"\\nQuery: {query}\")\n    result = route_query(query, \"ANALYTICS\")\n    print(f\"Result: {result['result']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:07:57.562748Z","iopub.execute_input":"2025-11-29T07:07:57.563329Z","iopub.status.idle":"2025-11-29T07:08:02.284699Z","shell.execute_reply.started":"2025-11-29T07:07:57.563305Z","shell.execute_reply":"2025-11-29T07:08:02.283986Z"}},"outputs":[{"name":"stdout","text":"\n────────────────────────────────────────────────────────────────────────────────\nTEST 2: DATA ANALYTICS AGENT\n────────────────────────────────────────────────────────────────────────────────\n\nQuery: How many emails are in the dataset?\n\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `python_repl_ast` with `{'query': 'df.shape[0]'}`\n\n\n\u001b[0m\u001b[36;1m\u001b[1;3m517401\u001b[0m\u001b[32;1m\u001b[1;3mThere are 517,401 emails in the dataset.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\nResult: There are 517,401 emails in the dataset.\n\nQuery: What is the average email body length?\n\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `python_repl_ast` with `{'query': \"average_body_length = df['body_length'].mean()\\naverage_body_length\"}`\n\n\n\u001b[0m\u001b[36;1m\u001b[1;3m1826.8477911716445\u001b[0m\u001b[32;1m\u001b[1;3mThe average email body length is approximately 1826.85 characters.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\nResult: The average email body length is approximately 1826.85 characters.\n\nQuery: Show me the top 3 most frequent senders\n\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `python_repl_ast` with `{'query': \"df['from'].value_counts().head(3)\"}`\n\n\n\u001b[0m\u001b[36;1m\u001b[1;3mfrom\nkay.mann@enron.com          16735\nvince.kaminski@enron.com    14368\njeff.dasovich@enron.com     11411\nName: count, dtype: int64\u001b[0m\u001b[32;1m\u001b[1;3mThe top 3 most frequent senders and the count of their messages are:\n1. kay.mann@enron.com - 16735 messages\n2. vince.kaminski@enron.com - 14368 messages\n3. jeff.dasovich@enron.com - 11411 messages\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\nResult: The top 3 most frequent senders and the count of their messages are:\n1. kay.mann@enron.com - 16735 messages\n2. vince.kaminski@enron.com - 14368 messages\n3. jeff.dasovich@enron.com - 11411 messages\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# Test 3: SQL Agent\nprint(\"\\n\" + \"─\"*80)\nprint(\"TEST 3: SQL AGENT\")\nprint(\"─\"*80)\n\n# Test 1: Total email count\nprint(\"TOTAL EMAIL COUNT\")\nprint(\"-\" * 40)\n\nsql_query_1 = \"\"\"\nSELECT COUNT(*) AS total_emails\nFROM emails;\n\"\"\"\n\nresult1 = route_query(sql_query_1, \"SQL\")\nprint(f\" SQL:\\n{sql_query_1}\\n\")\nprint(\"Total Email Count - SQL Result:\")\nprint(result1)\n\n# Test 2: Top 5 senders by email count\nprint(\"\\nTOP SENDERS BY EMAIL COUNT\")\nprint(\"-\" * 40)\n\nsql_query_2 = \"\"\"\nSELECT \"from\", COUNT(*) AS email_count\nFROM emails\nGROUP BY \"from\"\nORDER BY email_count DESC\nLIMIT 5;\n\"\"\"\n\nresult2 = route_query(sql_query_2, \"SQL\")\nprint(f\" SQL:\\n{sql_query_2}\\n\")\nprint(\"TOP SENDERS BY EMAIL COUNT - SQL Result:\")\nprint(result2)\nprint()\n\n# Test 3: Emails containing \"meeting\" in subject (case-insensitive)\nprint(\"\\nEMAILS ABOUT MEETINGS\")\nprint(\"-\" * 40)\n\nsql_query_3 = \"\"\"\nSELECT file_ref, \"from\", subject, date\nFROM emails\nWHERE UPPER(subject) LIKE '%MEETING%'\nORDER BY date DESC\nLIMIT 10;\n\"\"\"\n\nresult3 = route_query(sql_query_3, \"SQL\")\nprint(f\" SQL:\\n{sql_query_3}\\n\")\nprint(\"EMAILS ABOUT MEETINGS - SQL Result:\")\nif not result3.empty:\n    for idx, row in result3.iterrows():\n        print(f\"📧 {row['from']} | {row['subject'][:60]}... | {row['date'][:19]}\")\nelse:\n    print(\"No emails found with 'meeting' in subject\")\nprint()\n\nprint(\"\\n\" + \"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:09:19.006821Z","iopub.execute_input":"2025-11-29T07:09:19.007409Z","iopub.status.idle":"2025-11-29T07:09:19.083316Z","shell.execute_reply.started":"2025-11-29T07:09:19.007386Z","shell.execute_reply":"2025-11-29T07:09:19.082709Z"}},"outputs":[{"name":"stdout","text":"\n────────────────────────────────────────────────────────────────────────────────\nTEST 3: SQL AGENT\n────────────────────────────────────────────────────────────────────────────────\nTOTAL EMAIL COUNT\n----------------------------------------\n SQL:\n\nSELECT COUNT(*) AS total_emails\nFROM emails;\n\n\nTotal Email Count - SQL Result:\n   total_emails\n0        517401\n\nTOP SENDERS BY EMAIL COUNT\n----------------------------------------\n SQL:\n\nSELECT \"from\", COUNT(*) AS email_count\nFROM emails\nGROUP BY \"from\"\nORDER BY email_count DESC\nLIMIT 5;\n\n\nTOP SENDERS BY EMAIL COUNT - SQL Result:\n                       from  email_count\n0        kay.mann@enron.com        16735\n1  vince.kaminski@enron.com        14368\n2   jeff.dasovich@enron.com        11411\n3      pete.davis@enron.com         9149\n4   chris.germany@enron.com         8801\n\n\nEMAILS ABOUT MEETINGS\n----------------------------------------\n SQL:\n\nSELECT file_ref, \"from\", subject, date\nFROM emails\nWHERE UPPER(subject) LIKE '%MEETING%'\nORDER BY date DESC\nLIMIT 10;\n\n\nEMAILS ABOUT MEETINGS - SQL Result:\n📧 bruce.smith@enron.com | Elevator meeting... | Wed, 9 May 2001 19:\n📧 stephanie.harris@enron.com | Meeting of the Association of General Counsel - The Greenbri... | Wed, 9 May 2001 17:\n📧 stephanie.harris@enron.com | Meeting of the Association of General Counsel - The Greenbri... | Wed, 9 May 2001 17:\n📧 ann_rubin@enron.com | Meeting with Mr. Idei... | Wed, 9 May 2001 16:\n📧 hector.mcloughlin@enron.com | RE: PRC Meeting Update... | Wed, 9 May 2001 15:\n📧 billy.dorsey@enron.com | re: Executive Committee Meeting- Monday, May 14th... | Wed, 9 May 2001 14:\n📧 bwoertz@caiso.com | CAISO NOTICE: Cancellation of the May 10, 2001 EOB/CASIO Gen... | Wed, 9 May 2001 14:\n📧 airam.arteaga@enron.com | Meeting-Devon/Panaco Penalty Cash out... | Wed, 9 May 2001 13:\n📧 airam.arteaga@enron.com | Meeting-Devon/Panaco Penalty Cash out... | Wed, 9 May 2001 13:\n📧 patti.thompson@enron.com | Luncheon Meeting to Revise Revenue Model... | Wed, 9 May 2001 13:\n\n\n================================================================================\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"#### Interactive mode with agent selection","metadata":{}},{"cell_type":"code","source":"def interactive_multi_agent_mode():\n    \"\"\"\n    Interactive mode with agent selection\n\n    \"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"INTERACTIVE MULTI-AGENT MODE\")\n    print(\"=\"*80)\n    print(\"\\nAvailable Commands:\")\n    print(\"  /mode [conv|analytics|sql] - Switch agent mode\")\n    print(\"  /info - Show agent information\")\n    print(\"  /history - Show conversation history (conversational mode)\")    \n    print(\"  /exit - Exit system\")\n    print(\"=\"*80)\n    \n    agent_info = get_agent_info()    \n    print(f\"Available Agents: {', '.join(agent_info['available_agents'])}\\n\")\n    current_mode = None\n    \n    while True:\n        try:\n            user_input = input(\"\\n> \").strip()\n            \n            if not user_input:\n                continue\n            \n            # Handle commands\n            if user_input.startswith('/'):\n                if user_input == '/exit':\n                    print(\"Goodbye!\")\n                    break\n                    \n                elif user_input == '/info':\n                    info = get_agent_info()\n                    print(f\"Available Agents: {', '.join(info['available_agents'])}\")\n                    for agent, capability in info['capabilities'].items():\n                        print(f\"  • {agent}: {capability}\")\n                    \n                elif user_input.startswith('/mode'):\n                    parts = user_input.split()\n                    if len(parts) < 2:\n                        print(\"Usage: /mode [conv|analytics|sql]\")\n                        continue\n                    \n                    mode_map = {\n                        'conv': \"CONVERSATIONAL\",                        \n                        'analytics': \"ANALYTICS\",\n                        'sql': \"SQL\"\n                    }\n                    \n                    mode = mode_map.get(parts[1])\n                    \n                    if mode:\n                        current_mode = mode\n                        print(\"Current Agent Mode: \", current_mode)\n                        print(\"Current Agent: \", agents[current_mode])\n                    else:\n                        print(\"Invalid mode. Use: conv, analytics, or sql\")\n                \n                elif user_input == '/history':\n                    if current_mode == \"CONVERSATIONAL\":\n                        history = get_conversation_history()\n                        print(\"\\nConversation History:\")\n                        for turn in history:\n                            print(f\"{turn['role']}: {turn['content'][:100]}...\")\n                    else:\n                        print(\"History only available in conversational mode\")                                                                      \n                else:\n                    print(\"Unknown command. Type /exit to quit.\")\n                \n                continue\n            \n            # Route query to current agent\n            print(\"\\nProcessing...\")\n            result = route_query(user_input, current_mode)\n            \n            if result.get('status') == 'error':\n                print(f\"Error: {result.get('error')}\")\n                continue\n            \n            # Display results based on agent type\n            if current_mode == \"CONVERSATIONAL\":\n                print(f\"User Input:  {user_input}\\n\")\n                text = result.get(\"response\", result.get(\"answer\", result))\n                print(f\"CONVERSATIONAL Agent: {text}\\n\")\n\n                if result.get(\"source_documents\"):\n                    print(f\"  [Referenced {len(result['source_documents'])} emails]\")\n                    \n            elif current_mode == \"ANALYTICS\":\n                print(f\"User Input:  {user_input}\")\n                print(f\"\\n ANALYTICS Agent: {result['result']}\")\n                \n            elif current_mode == \"SQL\":\n                print(f\"User Input:  {user_input}\")\n                print(f\"\\n SQL Agent: {result}\")\n        \n        except KeyboardInterrupt:\n            print(\"\\n\\nExiting...\")\n            break\n        except Exception as e:\n            print(f\"\\nError: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:16:15.361050Z","iopub.execute_input":"2025-11-29T07:16:15.361318Z","iopub.status.idle":"2025-11-29T07:16:15.371384Z","shell.execute_reply.started":"2025-11-29T07:16:15.361300Z","shell.execute_reply":"2025-11-29T07:16:15.370804Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"interactive_multi_agent_mode()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:16:18.185474Z","iopub.execute_input":"2025-11-29T07:16:18.185739Z","iopub.status.idle":"2025-11-29T07:18:07.496991Z","shell.execute_reply.started":"2025-11-29T07:16:18.185719Z","shell.execute_reply":"2025-11-29T07:18:07.496292Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nINTERACTIVE MULTI-AGENT MODE\n================================================================================\n\nAvailable Commands:\n  /mode [conv|analytics|sql] - Switch agent mode\n  /info - Show agent information\n  /history - Show conversation history (conversational mode)\n  /exit - Exit system\n================================================================================\nAvailable Agents: CONVERSATIONAL, ANALYTICS, SQL\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n>  /info\n"},{"name":"stdout","text":"Available Agents: CONVERSATIONAL, ANALYTICS, SQL\n  • conversational: Natural conversation with memory\n  • analytics: Data analysis with pandas\n  • sql: SQL queries and structured data retrieval\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n>  /mode analytics\n"},{"name":"stdout","text":"Current Agent Mode:  ANALYTICS\nCurrent Agent:  analytics_agent\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n>  Show me the top 3 most frequent senders\n"},{"name":"stdout","text":"\nProcessing...\n\n\n\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `python_repl_ast` with `{'query': \"df['from'].value_counts().head(3)\"}`\n\n\n\u001b[0m\u001b[36;1m\u001b[1;3mfrom\nkay.mann@enron.com          16735\nvince.kaminski@enron.com    14368\njeff.dasovich@enron.com     11411\nName: count, dtype: int64\u001b[0m\u001b[32;1m\u001b[1;3mThe top 3 most frequent senders and the number of emails they sent are:\n1. kay.mann@enron.com - 16,735 emails\n2. vince.kaminski@enron.com - 14,368 emails\n3. jeff.dasovich@enron.com - 11,411 emails\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\nUser Input:  Show me the top 3 most frequent senders\n\n ANALYTICS Agent: The top 3 most frequent senders and the number of emails they sent are:\n1. kay.mann@enron.com - 16,735 emails\n2. vince.kaminski@enron.com - 14,368 emails\n3. jeff.dasovich@enron.com - 11,411 emails\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n>  /mode sql\n"},{"name":"stdout","text":"Current Agent Mode:  SQL\nCurrent Agent:  sql_agent\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n>  SELECT COUNT(*) AS total_emails FROM emails;\n"},{"name":"stdout","text":"\nProcessing...\nUser Input:  SELECT COUNT(*) AS total_emails FROM emails;\n\n SQL Agent:    total_emails\n0        517401\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n>  /mode conv\n"},{"name":"stdout","text":"Current Agent Mode:  CONVERSATIONAL\nCurrent Agent:  conversation_chain\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n>  I'm looking for emails about business meetings or travel plans.\n"},{"name":"stdout","text":"\nProcessing...\nUser Input:  I'm looking for emails about business meetings or travel plans.\n\nCONVERSATIONAL Agent: Certainly! To help narrow down the search for emails related to business meetings or travel plans, could you provide me with any specific details or keywords? For instance, do you remember any specific dates, attendees, destinations, or topics discussed in those emails? This information will assist me in finding the relevant emails more efficiently. Feel free to share any details you recall, and I'll do my best to assist you further!\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n>  provide email details with context as 'Elevator meeting'\n"},{"name":"stdout","text":"\nProcessing...\nUser Input:  provide email details with context as 'Elevator meeting'\n\nCONVERSATIONAL Agent: Certainly! To find emails with the context of 'Elevator meeting,' I can search for specific details related to that topic. Do you recall any specific dates, senders, or key points discussed during these elevator meetings that could help narrow down the search? Feel free to provide any additional information, and I'll do my best to locate the relevant emails for you. I'm here to assist you in finding the information you need!\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n>  provide the email data of meeting happend on Wed, 9 May 2001 19\n"},{"name":"stdout","text":"\nProcessing...\nUser Input:  provide the email data of meeting happend on Wed, 9 May 2001 19\n\nCONVERSATIONAL Agent: Certainly! I can help you find the email data for the meeting that took place on Wednesday, May 9, 2001, at 7:00 PM. In the email from Sarah on May 10th, there are detailed notes and action items discussed during that meeting. Would you like me to provide you with a summary of the key points or any specific information from that email? Feel free to let me know how I can assist you further!\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n>  /history\n"},{"name":"stdout","text":"\nConversation History:\nhuman: Hi! Can you help me search my emails?...\nai: Of course! I'd be happy to help you search through your emails. Could you please provide me with som...\nhuman: I'm looking for emails about business meetings or travel plans....\nai: Of course! I'd be happy to help you search for emails about business meetings or travel plans. Could...\nhuman: What about salary discussions?...\nai: I see you're looking for emails about business meetings or travel plans. Are you also interested in ...\nhuman: Hi! Can you help me search my emails?...\nai: Certainly! To narrow down the search for emails related to salary discussions, could you provide me ...\nhuman: I'm looking for emails about business meetings or travel plans....\nai: Certainly! If you're interested in emails related to salary discussions, I can help you search for t...\nhuman: What about salary discussions?...\nai: Certainly! I can help you search for emails related to salary discussions as well. Do you recall any...\nhuman: I'm looking for emails about business meetings or travel plans....\nai: Certainly! To help narrow down the search for emails related to business meetings or travel plans, c...\nhuman: provide email details with context as 'Elevator meeting'...\nai: Certainly! To find emails with the context of 'Elevator meeting,' I can search for specific details ...\nhuman: provide the email data of meeting happend on Wed, 9 May 2001 19...\nai: Certainly! I can help you find the email data for the meeting that took place on Wednesday, May 9, 2...\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\n>  /exit\n"},{"name":"stdout","text":"Goodbye!\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}